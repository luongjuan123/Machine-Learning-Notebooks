{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d289acc-52ef-4aad-a62d-d9b926e2455e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fcadf3b-4c34-4cff-8265-379ea68a5110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class util(object):\n",
    "    @staticmethod\n",
    "    def add_intercept(x):\n",
    "        \"\"\"Add intercept to matrix x.\n",
    "\n",
    "        Args:\n",
    "            x: 2D NumPy array.\n",
    "\n",
    "        Returns:\n",
    "            New matrix same as x with 1's in the 0th column.\n",
    "        \"\"\"\n",
    "        new_x = np.zeros((x.shape[0], x.shape[1] + 1), dtype=x.dtype)\n",
    "        new_x[:, 0] = 1\n",
    "        new_x[:, 1:] = x\n",
    "        return new_x\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(csv_path, label_col='y', add_intercept=False):\n",
    "        \"\"\"Load dataset from a CSV file.\n",
    "\n",
    "        Args:\n",
    "            csv_path: Path to CSV file containing dataset.\n",
    "            label_col: Name of column to use as labels (should be 'y' or 't').\n",
    "            add_intercept: Add an intercept entry to x-values.\n",
    "\n",
    "        Returns:\n",
    "            xs: Numpy array of x-values (inputs).\n",
    "            ys: Numpy array of y-values (labels).\n",
    "        \"\"\"\n",
    "        # Validate label_col argument\n",
    "        allowed_label_cols = ('y', 't')\n",
    "        if label_col not in allowed_label_cols:\n",
    "            raise ValueError(\n",
    "                f'Invalid label_col: {label_col} (expected one of {allowed_label_cols})'\n",
    "            )\n",
    "\n",
    "        # Load headers\n",
    "        with open(csv_path, 'r') as csv_fh:\n",
    "            headers = csv_fh.readline().strip().split(',')\n",
    "\n",
    "        # Load features and labels\n",
    "        x_cols = [i for i in range(len(headers)) if headers[i].startswith('x')]\n",
    "        l_cols = [i for i in range(len(headers)) if headers[i] == label_col]\n",
    "        inputs = np.loadtxt(csv_path, delimiter=',', skiprows=1, usecols=x_cols)\n",
    "        labels = np.loadtxt(csv_path, delimiter=',', skiprows=1, usecols=l_cols)\n",
    "\n",
    "        if inputs.ndim == 1:\n",
    "            inputs = np.expand_dims(inputs, -1)\n",
    "\n",
    "        if add_intercept:\n",
    "            inputs = util.add_intercept(inputs)\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def plot(x, y, theta, save_path=None, correction=1.0):\n",
    "        \"\"\"Plot dataset and fitted logistic regression parameters.\n",
    "\n",
    "        Args:\n",
    "            x: Matrix of training examples, one per row.\n",
    "            y: Vector of labels in {0, 1}.\n",
    "            theta: Vector of parameters for logistic regression model.\n",
    "            save_path: Path to save the plot.\n",
    "            correction: Correction factor to apply (Problem 2(e) only).\n",
    "        \"\"\"\n",
    "        # Plot dataset\n",
    "        plt.figure()\n",
    "        plt.plot(x[y == 1, -2], x[y == 1, -1], 'bx', linewidth=2)\n",
    "        plt.plot(x[y == 0, -2], x[y == 0, -1], 'go', linewidth=2)\n",
    "\n",
    "        # Plot decision boundary (found by solving for theta^T x = 0)\n",
    "        margin1 = (max(x[:, -2]) - min(x[:, -2])) * 0.2\n",
    "        margin2 = (max(x[:, -1]) - min(x[:, -1])) * 0.2\n",
    "        x1 = np.arange(min(x[:, -2]) - margin1, max(x[:, -2]) + margin1, 0.01)\n",
    "        x2 = -(theta[0] / theta[2] * correction + theta[1] / theta[2] * x1)\n",
    "        plt.plot(x1, x2, c='red', linewidth=2)\n",
    "        plt.xlim(x[:, -2].min() - margin1, x[:, -2].max() + margin1)\n",
    "        plt.ylim(x[:, -1].min() - margin2, x[:, -1].max() + margin2)\n",
    "\n",
    "        # Add labels and save to disk\n",
    "        plt.xlabel('x1')\n",
    "        plt.ylabel('x2')\n",
    "        if save_path is not None:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fefe367-11d8-435e-9c3a-0ec03aa41813",
   "metadata": {},
   "source": [
    "# **Linear Regression**\n",
    "\n",
    "---\n",
    "\n",
    "## **Loss (Cost) Function**\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{\\theta}) = \\frac{1}{2} \n",
    "\\sum_{i=1}^{m} \\left( h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Hypothesis Function**\n",
    "\n",
    "$$\n",
    "h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) = \n",
    "\\boldsymbol{\\theta}^\\top \\mathbf{x}^{(i)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Notation Summary**\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|:--:|:--|\n",
    "| $m$ | number of training examples |\n",
    "| $\\mathbf{x}^{(i)}$ | input feature vector of the $i^{th}$ example |\n",
    "| $y^{(i)}$ | true label for the $i^{th}$ example |\n",
    "| $\\boldsymbol{\\theta}$ | parameter (weight) vector |\n",
    "| $h_{\\boldsymbol{\\theta}}(x)$ | predicted value (hypothesis) |\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ’¡ *Goal:* Find parameters $\\boldsymbol{\\theta}$ that minimize the cost function $J(\\boldsymbol{\\theta})$, i.e.  \n",
    "$$\n",
    "\\boldsymbol{\\theta}^* = \\arg\\min_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c408a3aa-2e0a-4fdb-96c2-4005d430fd7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LinearModel(object):\n",
    "    \"\"\"Base class for linear models\"\"\"\n",
    "    def __init__(self, step_size, max_iter=100, eps=1e-6,\n",
    "                theta_0=None, verbose=True):\n",
    "        \n",
    "        self.theta = theta_0\n",
    "        self.step_size = step_size\n",
    "        self.max_iter = max_iter\n",
    "        self.eps = eps\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(X, y):\n",
    "        \"\"\"Run solver so fit linear model.\n",
    "        \n",
    "        Args:\n",
    "            X: Training example inputs. Shape (m, n)\n",
    "            y: Training example labels. Shape (m, )\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Subclass of LinearModel must implement fit method.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        \"\"\"Make a pediction given new inputs x\n",
    "        Args:\n",
    "            X: Input of shape(m, n)\n",
    "        \n",
    "        Returns:\n",
    "            Output of shape(m, )\"\"\"\n",
    "\n",
    "        raise NotImplemetedError(\"Subclass of LinearModel must implement predict method.\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71de0dd5-12b4-41c3-bfa9-662a7d267b6b",
   "metadata": {},
   "source": [
    "# **1. Batch Gradient Descent(BGD)**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Repeat until convergence {\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j + \\alpha \\sum_{i=1}^{m} (y^{(i)} - h_{\\theta}(x^{(i)})) x_j^{(i)} \\quad \\text{(for every j)}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d80a74da-efa0-4fa4-b28a-8e49db0c5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegessionBatchGradientDescent(LinearModel):\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.theta = np.zeros(n)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            theta_old = np.copy(self.theta)\n",
    "            \n",
    "            # Compute gradient\n",
    "            gradient = (1/(2*m)) * (X @ self.theta - y) @ X\n",
    "            \n",
    "            # Update parameters\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(self.theta - theta_old, ord=1) < self.eps:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.theta\n",
    "        \n",
    "def main(train_path, eval_path):\n",
    "    print(\"YES\")\n",
    "    X_train, y_train = util.load_dataset(train_path, add_intercept = True)\n",
    "\n",
    "    model = LinearRegressionBatchGradientDescent()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    X_eval, y_eval = util.load_dataset(eval_path, add_intercept = True)\n",
    "\n",
    "    y_pred = model.predict(X_eval)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40391e3-7676-48bd-8fad-03adda4448a2",
   "metadata": {},
   "source": [
    "# **2. Stochatic Gradient Descent(BGD)**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Loop {\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i=1 to m, {\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j + \\alpha (y^{(i)} - h_{\\theta}(x^{(i)})) x_j^{(i)} \\quad \\text{(for every j)}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "282f22fe-c421-49fe-a22a-59c425669aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionStochaticGradientDescent(LinearModel):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape()\n",
    "\n",
    "        #Initialize parameters\n",
    "        self.theta = np.zeros(n)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            theta_old = np.copy(self.theta)\n",
    "\n",
    "            # Shuffle data each epoch\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            # Update weights per sample\n",
    "            for i in range(m):\n",
    "                xi = X_shuffled[i:i+1]          # shape (1, n)\n",
    "                yi = y_shuffled[i]\n",
    "                gradient = 1/2 * (xi @ self.theta - yi) @ xi\n",
    "                self.theta -= self.learning_rate * gradient.flatten()\n",
    "\n",
    "            # Early stopping if change small\n",
    "            if np.linalg.norm(self.theta - theta_old, ord=1) < self.tol:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.theta        \n",
    "\n",
    "def main(train_path, eval_path):\n",
    "    pass        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc7761-1653-4288-9440-d5bf97f60114",
   "metadata": {},
   "source": [
    "# **3. Normal Equation**\n",
    "### **3.1 Matrix derivatives**\n",
    "For a function $f: \\mathbb{R}^{m \\times n} \\mapsto \\mathbb{R}$ mapping from m-by-n matrices to the real numbers, we define the derivative of $f$ with respect to $A$ to be:\n",
    "\n",
    "$$\n",
    "\\nabla_A f(A) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial A_{11}} & \\dots & \\frac{\\partial f}{\\partial A_{1n}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial A_{m1}} & \\dots & \\frac{\\partial f}{\\partial A_{mn}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "### **3.2 Least squares revisited**\n",
    "\n",
    "Given a training setm define the **design matrix X** to be the m-by-n matrix(actually m-by-n + 1 if we include the intercept term) that containsthe training examples' input values in its rows:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "â€” (x^{(1)})^T â€” \\\\\n",
    "â€” (x^{(2)})^T â€” \\\\\n",
    "\\vdots \\\\\n",
    "â€” (x^{(m)})^T â€”\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Also, let $\\vec{y}$ be the $m$-dimensional vector containing all the target values from the training set:\n",
    "\n",
    "$$\n",
    "\\vec{y} =\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, since $h_{\\theta}(x^{(i)}) = (x^{(i)})^T \\theta$, we can easily verify that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X\\theta - \\vec{y} &=\n",
    "\\begin{bmatrix}\n",
    "(x^{(1)})^T \\theta \\\\\n",
    "\\vdots \\\\\n",
    "(x^{(m)})^T \\theta\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "y^{(m)}\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "h_{\\theta}(x^{(1)}) - y^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "h_{\\theta}(x^{(m)}) - y^{(m)}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, using the fact that for a vector $z$, we have that $z^T z = \\sum_i z_i^2$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{2} (X\\theta - \\vec{y})^T (X\\theta - \\vec{y}) &= \\frac{1}{2} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2 \\\\\n",
    "&= J(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finally, to minimize $J$, let's find its derivatives with respect to $\\theta$. \n",
    "\n",
    "$$\n",
    "\\nabla_A \\text{tr} ABA^T C = B^T A^T C^T + B A^T C \\quad (5)\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\theta} J(\\theta) &= \\nabla_{\\theta} \\frac{1}{2} (X\\theta - \\vec{y})^T (X\\theta - \\vec{y}) \\\\\n",
    "&= \\frac{1}{2} \\nabla_{\\theta} (\\theta^T X^T X \\theta - \\theta^T X^T \\vec{y} - \\vec{y}^T X \\theta + \\vec{y}^T \\vec{y}) \\\\\n",
    "&= \\frac{1}{2} \\nabla_{\\theta} \\text{tr} (\\theta^T X^T X \\theta - \\theta^T X^T \\vec{y} - \\vec{y}^T X \\theta + \\vec{y}^T \\vec{y}) \\\\\n",
    "&= \\frac{1}{2} \\nabla_{\\theta} (\\text{tr} \\theta^T X^T X \\theta - 2 \\text{tr} \\vec{y}^T X \\theta) \\\\\n",
    "&= \\frac{1}{2} (X^T X \\theta + X^T X \\theta - 2 X^T \\vec{y}) \\\\\n",
    "&= X^T X \\theta - X^T \\vec{y}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the third step, we used the fact that the trace of a real number is just the real number; the fourth step used the fact that $\\text{tr} A = \\text{tr} A^T$, and the fifth step used Equation (5) with $A^T = \\theta$, $B = B^T = X^T X$, and $C = I$, and Equation (1). To minimize $J$, we set its derivatives to zero, and obtain the **normal equations**:\n",
    "\n",
    "$$\n",
    "X^T X \\theta = X^T \\vec{y}\n",
    "$$\n",
    "\n",
    "Thus, the value of $\\theta$ that minimizes $J(\\theta)$ is given in closed form by the equation\n",
    "\n",
    "**$$\n",
    "\\theta = (X^T X)^{-1} X^T \\vec{y}.\n",
    "$$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989aca5e-1df8-4806-aa06-04e18fe49d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionNormalEquation(LinearModel):\n",
    "    def fit(self, X, y):\n",
    "        # Normal equation: Î¸ = (Xáµ€X)â»Â¹ Xáµ€y\n",
    "        self.theta = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.theta\n",
    "def main(train_path, eval_path):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1c2045-758a-4a4b-b2c4-bf9bc283a720",
   "metadata": {},
   "source": [
    "# **4. Locally Weighted Linear Regression**\n",
    "The locally weighted linear regression algorithm does the following:\n",
    "\n",
    "1.  Fit $\\theta$ to minimize $\\sum_{i} w^{(i)} (y^{(i)} - \\theta^T x^{(i)})^2$.\n",
    "2.  Output $\\theta^T x$.\n",
    "\n",
    "A fairly standard choice for the weights is\n",
    "\n",
    "$$\n",
    "w^{(i)} = \\exp \\left( - \\frac{(x^{(i)} - x)^2}{2\\tau^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ae0c51-2d8f-4370-9e35-353fd2010b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocallyWeightedLinearRegression:\n",
    "    def __init__(self, tau):\n",
    "        self.tau = tau\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Store training data.\"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions given inputs X.\n",
    "        \n",
    "        Args:\n",
    "            X: shape (m, n)\n",
    "        Returns:\n",
    "            y_pred: shape (m,)\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        y_pred = np.zeros(m)\n",
    "\n",
    "        for i in range(m):\n",
    "            # compute weights for each training sample relative to query X[i]\n",
    "            diff = self.X - X[i]                     # (m, n)\n",
    "            W = np.diag(np.exp(-np.sum(diff**2, axis=1) / (2 * self.tau**2)))\n",
    "\n",
    "            # solve for theta = (X^T W X)^(-1) X^T W y\n",
    "            A = self.x.T @ W @ self.x\n",
    "            b = self.x.T @ W @ self.y\n",
    "            # regularization for stability\n",
    "            theta = np.linalg.solve(A + 1e-5 * np.eye(n), b)\n",
    "\n",
    "            # predict y_hat = x_i^T theta\n",
    "            y_pred[i] = X[i] @ theta\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "def main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
